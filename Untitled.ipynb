{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8e541c-adc1-4ac9-a76f-06408de892f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "#load data\n",
    "data = np.load('octave2_x_T.npy')\n",
    "prev_data = np.load('octave2_prev_x_T.npy')\n",
    "print('data shape: {}'.format(data.shape))\n",
    "time.sleep(3)\n",
    "\n",
    "song_idx = int(data.shape[0]/8)\n",
    "test_ratial = 0.1\n",
    "test_song_num = round(song_idx*test_ratial)\n",
    "train_song_num = data.shape[0] - test_song_num\n",
    "print('total song number: {}'.format(song_idx))\n",
    "print('number of test song: {}, \\n,number of train song: {}'.format(test_song_num,train_song_num))\n",
    "time.sleep(3)\n",
    "\n",
    "#create the song idx for test data\n",
    "\n",
    "full = np.arange(song_idx)\n",
    "\n",
    "test_idx= random.sample(range(0,full.shape[0]),test_song_num)\n",
    "test_idx = np.asarray(test_idx)\n",
    "print('total {} song idx for test: {}'.format(test_idx.shape[0],test_idx))\n",
    "time.sleep(3)\n",
    "\n",
    "#create the song idx for train data\n",
    "train_idx = np.delete(full,test_idx)\n",
    "print('total {} song idx for train: {}'.format(train_idx.shape[0],train_idx))\n",
    "time.sleep(3)\n",
    "\n",
    "    \n",
    "\n",
    "def test_data(data,test_idx):\n",
    "\n",
    "    #save the test data and train data separately\n",
    "    X_te = []\n",
    "    for i in range(test_idx.shape[0]):\n",
    "        stp = (test_idx[i])*8\n",
    "        edp = stp + 8\n",
    "        song = data[stp:edp,0,:,:]\n",
    "        song = song.reshape((8,1,128,16))\n",
    "        X_te.append(song)\n",
    "        # print('i: {}, test_iex: {}, stp: {}, song.shape: {}, song num: {}'.format(i, test_idx[i], stp, song.shape, len(X_te)))\n",
    "        \n",
    "    X_te = np.vstack(X_te)\n",
    "    return X_te\n",
    "\n",
    "\n",
    "def train_data(data,train_idx):\n",
    "\n",
    "    #save the test data and train data separately\n",
    "    X_tr = []\n",
    "    for i in range(train_idx.shape[0]):\n",
    "        stp = (train_idx[i])*8\n",
    "        edp = stp + 8\n",
    "        song = data[stp:edp,0,:,:]\n",
    "        song = song.reshape((8,1,128,16))\n",
    "        X_tr.append(song)\n",
    "        # print('i: {}, train_iex: {}, stp: {}, song.shape: {}, song num: {}'.format(i, train_idx[i], stp, song.shape, len(X_tr)))\n",
    "\n",
    "    X_tr = np.vstack(X_tr)\n",
    "    return X_tr\n",
    "\n",
    "\n",
    "\n",
    "# test_data\n",
    "X_te = test_data(data,test_idx)\n",
    "prev_X_te = test_data(prev_data,test_idx)\n",
    "np.save('octave2_X_te.npy',X_te)\n",
    "np.save('octave2_prev_X_te',prev_X_te)\n",
    "\n",
    "print('test song completed, X_te matrix shape: {}'.format(X_te.shape))\n",
    "\n",
    "#train_data\n",
    "X_tr = train_data(data,train_idx)\n",
    "prev_X_tr = train_data(prev_data,train_idx)\n",
    "np.save('octave2_X_tr.npy',X_tr)\n",
    "np.save('octave2_prev_X_tr.npy',prev_X_tr)\n",
    "\n",
    "print('train song completed, X_tr matrix shape: {}'.format(X_tr.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "065df7c5-f1a0-46ab-93c7-a68f9c2fe629",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from ops import *\n",
    "\n",
    "\n",
    "class sample_generator(nn.Module):\n",
    "    def __init__(self, pitch_range):\n",
    "        super(sample_generator, self).__init__()\n",
    "        self.gf_dim   = 64\n",
    "        self.y_dim   = 13\n",
    "        self.n_channel = 256\n",
    "\n",
    "        self.h1      = nn.ConvTranspose2d(in_channels=157, out_channels=pitch_range, kernel_size=(2,1), stride=(2,2))\n",
    "        self.h2      = nn.ConvTranspose2d(in_channels=157, out_channels=pitch_range, kernel_size=(2,1), stride=(2,2))\n",
    "        self.h3      = nn.ConvTranspose2d(in_channels=157, out_channels=pitch_range, kernel_size=(2,1), stride=(2,2))\n",
    "        self.h4      = nn.ConvTranspose2d(in_channels=157, out_channels=1, kernel_size=(1,pitch_range), stride=(1,2))\n",
    "\n",
    "        self.h0_prev = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(1,pitch_range), stride=(1,2))\n",
    "        self.h1_prev = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(2,1), stride=(2,2))\n",
    "        self.h2_prev = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(2,1), stride=(2,2))\n",
    "        self.h3_prev = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(2,1), stride=(2,2))\n",
    "\n",
    "        self.linear1 = nn.Linear(113,1024)\n",
    "        self.linear2 = nn.Linear(1037,self.gf_dim*2*2*1)\n",
    "\n",
    "    def forward(self, z, prev_x, y ,batch_size,pitch_range):\n",
    "\n",
    "        # h3_prev = F.leaky_relu(self.batch_nor_256(self.h0_prev(prev_x)),0.2)\n",
    "        h0_prev = lrelu(batch_norm_2d_cpu(self.h0_prev(prev_x)),0.2)   #[72, 16, 16, 1]\n",
    "        h1_prev = lrelu(batch_norm_2d_cpu(self.h1_prev(h0_prev)),0.2)  #[72, 16, 8, 1]\n",
    "        h2_prev = lrelu(batch_norm_2d_cpu(self.h2_prev(h1_prev)),0.2)  #[72, 16, 4, 1]\n",
    "        h3_prev = lrelu(batch_norm_2d_cpu(self.h3_prev(h2_prev)),0.2)  #[72, 16, 2, 1])\n",
    "\n",
    "        yb = y.view(batch_size,  self.y_dim, 1, 1)  #(72,13,1,1)\n",
    "\n",
    "        z = torch.cat((z,y),1)         #(72,113)\n",
    "\n",
    "        h0 = F.relu(batch_norm_1d_cpu(self.linear1(z)))    #(72,1024)\n",
    "        h0 = torch.cat((h0,y),1)   #(72,1037)\n",
    "\n",
    "        h1 = F.relu(batch_norm_1d_cpu(self.linear2(h0)))   #(72, 256)\n",
    "        h1 = h1.view(batch_size, self.gf_dim * 2, 2, 1)     #(72,128,2,1)\n",
    "        h1 = conv_cond_concat(h1,yb) #(b,141,2,1)\n",
    "        h1 = conv_prev_concat(h1,h3_prev)  #(72, 157, 2, 1)\n",
    "\n",
    "        h2 = F.relu(batch_norm_2d_cpu(self.h1(h1)))  #(72, 128, 4, 1)\n",
    "        h2 = conv_cond_concat(h2,yb) #([72, 141, 4, 1])\n",
    "        h2 = conv_prev_concat(h2,h2_prev)  #([72, 157, 4, 1])\n",
    "\n",
    "        h3 = F.relu(batch_norm_2d_cpu(self.h2(h2)))  #([72, 128, 8, 1]) \n",
    "        h3 = conv_cond_concat(h3,yb)  #([72, 141, 8, 1])\n",
    "        h3 = conv_prev_concat(h3,h1_prev) #([72, 157, 8, 1])\n",
    "\n",
    "        h4 = F.relu(batch_norm_2d_cpu(self.h3(h3)))  #([72, 128, 16, 1])\n",
    "        h4 = conv_cond_concat(h4,yb)  #([72, 141, 16, 1])\n",
    "        h4 = conv_prev_concat(h4,h0_prev) #([72, 157, 16, 1])\n",
    "\n",
    "        g_x = torch.sigmoid(self.h4(h4)) #([72, 1, 16, 128])\n",
    "\n",
    "        return g_x\n",
    "\n",
    "\n",
    "\n",
    "class generator(nn.Module):\n",
    "    def __init__(self,pitch_range):\n",
    "        super(generator, self).__init__()\n",
    "        self.gf_dim   = 64\n",
    "        self.y_dim   = 13\n",
    "        self.n_channel = 256\n",
    "\n",
    "        self.h1      = nn.ConvTranspose2d(in_channels=157, out_channels=pitch_range, kernel_size=(2,1), stride=(2,2))\n",
    "        self.h2      = nn.ConvTranspose2d(in_channels=157, out_channels=pitch_range, kernel_size=(2,1), stride=(2,2))\n",
    "        self.h3      = nn.ConvTranspose2d(in_channels=157, out_channels=pitch_range, kernel_size=(2,1), stride=(2,2))\n",
    "        self.h4      = nn.ConvTranspose2d(in_channels=157, out_channels=1, kernel_size=(1,pitch_range), stride=(1,2))\n",
    "\n",
    "        self.h0_prev = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(1,pitch_range), stride=(1,2))\n",
    "        self.h1_prev = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(2,1), stride=(2,2))\n",
    "        self.h2_prev = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(2,1), stride=(2,2))\n",
    "        self.h3_prev = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(2,1), stride=(2,2))\n",
    "\n",
    "        self.linear1 = nn.Linear(113,1024)\n",
    "        self.linear2 = nn.Linear(1037,self.gf_dim*2*2*1)\n",
    "\n",
    "    def forward(self, z, prev_x, y ,batch_size,pitch_range):\n",
    "\n",
    "        # h3_prev = F.leaky_relu(self.batch_nor_256(self.h0_prev(prev_x)),0.2)\n",
    "        h0_prev = lrelu(batch_norm_2d(self.h0_prev(prev_x)),0.2)   #[72, 16, 16, 1]\n",
    "        h1_prev = lrelu(batch_norm_2d(self.h1_prev(h0_prev)),0.2)  #[72, 16, 8, 1]\n",
    "        h2_prev = lrelu(batch_norm_2d(self.h2_prev(h1_prev)),0.2)  #[72, 16, 4, 1]\n",
    "        h3_prev = lrelu(batch_norm_2d(self.h3_prev(h2_prev)),0.2)  #[72, 16, 2, 1])\n",
    "\n",
    "        yb = y.view(batch_size,  self.y_dim, 1, 1)  #(72,13,1,1)\n",
    "\n",
    "        z = torch.cat((z,y),1)         #(72,113)\n",
    "\n",
    "        h0 = F.relu(batch_norm_1d(self.linear1(z)))    #(72,1024)\n",
    "        h0 = torch.cat((h0,y),1)   #(72,1037)\n",
    "\n",
    "        h1 = F.relu(batch_norm_1d(self.linear2(h0)))   #(72, 256)\n",
    "        h1 = h1.view(batch_size, self.gf_dim * 2, 2, 1)     #(72,128,2,1)\n",
    "        h1 = conv_cond_concat(h1,yb) #(b,141,2,1)\n",
    "        h1 = conv_prev_concat(h1,h3_prev)  #(72, 157, 2, 1)\n",
    "\n",
    "        h2 = F.relu(batch_norm_2d(self.h1(h1)))  #(72, 128, 4, 1)\n",
    "        h2 = conv_cond_concat(h2,yb) #([72, 141, 4, 1])\n",
    "        h2 = conv_prev_concat(h2,h2_prev)  #([72, 157, 4, 1])\n",
    "\n",
    "        h3 = F.relu(batch_norm_2d(self.h2(h2)))  #([72, 128, 8, 1]) \n",
    "        h3 = conv_cond_concat(h3,yb)  #([72, 141, 8, 1])\n",
    "        h3 = conv_prev_concat(h3,h1_prev) #([72, 157, 8, 1])\n",
    "\n",
    "        h4 = F.relu(batch_norm_2d(self.h3(h3)))  #([72, 128, 16, 1])\n",
    "        h4 = conv_cond_concat(h4,yb)  #([72, 141, 16, 1])\n",
    "        h4 = conv_prev_concat(h4,h0_prev) #([72, 157, 16, 1])\n",
    "\n",
    "        g_x = torch.sigmoid(self.h4(h4)) #([72, 1, 16, 128])\n",
    "\n",
    "        return g_x\n",
    "\n",
    "\n",
    "class discriminator(nn.Module):\n",
    "    def __init__(self,pitch_range):\n",
    "        super(discriminator, self).__init__()\n",
    "\n",
    "        self.df_dim = 64\n",
    "        self.dfc_dim = 1024\n",
    "        self.y_dim = 13\n",
    "\n",
    "        self.h0_prev = nn.Conv2d(in_channels=14, out_channels=14, kernel_size=(2,pitch_range), stride=(2,2))\n",
    "        #out channels = y_dim +1 \n",
    "        self.h1_prev = nn.Conv2d(in_channels=27, out_channels=77, kernel_size=(4,1), stride=(2,2))\n",
    "        # out channels = df_dim + y_dim\n",
    "        self.linear1 = nn.Linear(244,self.dfc_dim)\n",
    "        self.linear2 = nn.Linear(1037,1)\n",
    "\n",
    "    def forward(self,x,y,batch_size,pitch_range):\n",
    "        yb = y.view(batch_size,self.y_dim, 1, 1)\n",
    "        x = conv_cond_concat(x, yb)  #x.shape torch.Size([72, 14, 16, 128])\n",
    "        \n",
    "        h0 = lrelu(self.h0_prev(x),0.2)\n",
    "        fm = h0\n",
    "        h0 = conv_cond_concat(h0, yb) #torch.Size([72, 27, 8, 1])\n",
    "\n",
    "        h1 = lrelu(batch_norm_2d(self.h1_prev(h0)),0.2)  #torch.Size([72, 77, 3, 1])\n",
    "        h1 = h1.view(batch_size, -1)  #torch.Size([72, 231])\n",
    "        h1 = torch.cat((h1,y),1)  #torch.Size([72, 244])\n",
    "\n",
    "        h2 = lrelu(batch_norm_1d(self.linear1(h1)))\n",
    "        h2 = torch.cat((h2,y),1)  #torch.Size([72, 1037])\n",
    "\n",
    "        h3 = self.linear2(h2)\n",
    "        h3_sigmoid = torch.sigmoid(h3)\n",
    "\n",
    "\n",
    "        return h3_sigmoid, h3, fm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6531d13-d7d1-4805-805e-98d2f3b9bc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data preparation is completed\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 43616, 161476, 170452, 157432) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\envs\\Midinet\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    989\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 990\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    991\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Midinet\\lib\\queue.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    178\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m                         \u001b[1;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-d35b68d48666>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_detect_anomaly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 290\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    291\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-d35b68d48666>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     95\u001b[0m             \u001b[0msum_D_x\u001b[0m   \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m             \u001b[0msum_D_G_z\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprev_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mchord\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m                 \u001b[1;31m############################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Midinet\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Midinet\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1186\u001b[1;33m             \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1187\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Midinet\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1140\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1142\u001b[1;33m                 \u001b[0msuccess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1143\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1144\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Midinet\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1001\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfailed_workers\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1002\u001b[0m                 \u001b[0mpids_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m', '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfailed_workers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1003\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'DataLoader worker (pid(s) {}) exited unexpectedly'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpids_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1004\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEmpty\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1005\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 43616, 161476, 170452, 157432) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from model import *\n",
    "from ops import *\n",
    "\n",
    "class get_dataloader(object):\n",
    "    def __init__(self, data, prev_data, y):\n",
    "        self.size = data.shape[0]\n",
    "        self.data = torch.from_numpy(data).float()\n",
    "        self.prev_data = torch.from_numpy(prev_data).float()\n",
    "        self.y   = torch.from_numpy(y).float()\n",
    "\n",
    "         # self.label = np.array(label)\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index],self.prev_data[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "def load_data():\n",
    "    #######load the data########\n",
    "    check_range_st = 0\n",
    "    check_range_ed = 129\n",
    "    pitch_range = check_range_ed - check_range_st-1\n",
    "    # print('pitch range: {}'.format(pitch_range))\n",
    "\n",
    "    X_tr = np.load('data/octave2_x.npy')\n",
    "    prev_X_tr = np.load('data/octave2_x.npy')\n",
    "    y_tr    = np.load('data/octave2_x.npy')\n",
    "    X_tr = X_tr[:,:,:,check_range_st:check_range_ed]\n",
    "    prev_X_tr = prev_X_tr[:,:,:,check_range_st:check_range_ed]\n",
    "\n",
    "    #test data shape(5048, 1, 16, 128)\n",
    "    #train data shape(45448, 1, 16, 128)\n",
    "\n",
    "    train_iter = get_dataloader(X_tr,prev_X_tr,y_tr)\n",
    "    kwargs = {'num_workers': 4, 'pin_memory': True}# if args.cuda else {}\n",
    "    train_loader = DataLoader(\n",
    "                   train_iter, batch_size=72, shuffle=True, **kwargs)\n",
    "\n",
    "    print('data preparation is completed')\n",
    "    #######################################\n",
    "    return train_loader\n",
    "\n",
    "def main():\n",
    "    is_train = 1\n",
    "    is_draw = 1\n",
    "    is_sample = 1\n",
    "\n",
    "    epochs = 20\n",
    "    lr = 0.0002\n",
    "\n",
    "    check_range_st = 0\n",
    "    check_range_ed = 129\n",
    "    pitch_range = check_range_ed - check_range_st-1\n",
    "    \n",
    "    device = torch.device('cuda')\n",
    "    train_loader = load_data()\n",
    "\n",
    "    if is_train == 1 :\n",
    "        netG = generator(pitch_range).to(device)\n",
    "        netD = discriminator(pitch_range).to(device)  \n",
    "\n",
    "        netD.train()\n",
    "        netG.train()\n",
    "        optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "        optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(0.5, 0.999)) \n",
    "             \n",
    "        batch_size = 72\n",
    "        nz = 100\n",
    "        fixed_noise = torch.randn(batch_size, nz, device=device)\n",
    "        real_label = 1\n",
    "        fake_label = 0\n",
    "        average_lossD = 0\n",
    "        average_lossG = 0\n",
    "        average_D_x   = 0\n",
    "        average_D_G_z = 0\n",
    "\n",
    "        lossD_list =  []\n",
    "        lossD_list_all = []\n",
    "        lossG_list =  []\n",
    "        lossG_list_all = []\n",
    "        D_x_list = []\n",
    "        D_G_z_list = []\n",
    "        for epoch in range(epochs):\n",
    "            sum_lossD = 0\n",
    "            sum_lossG = 0\n",
    "            sum_D_x   = 0\n",
    "            sum_D_G_z = 0\n",
    "            for i, (data,prev_data,chord) in enumerate(train_loader, 0):\n",
    "                \n",
    "                ############################\n",
    "                # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "                ###########################\n",
    "                # train with real\n",
    "                netD.zero_grad()\n",
    "                real_cpu = data.to(device)\n",
    "                prev_data_cpu = prev_data.to(device)\n",
    "                chord_cpu = chord.to(device)\n",
    "\n",
    "                batch_size = real_cpu.size(0)\n",
    "                label = torch.full((batch_size,), real_label, device=device, dtype=int)\n",
    "                D, D_logits, fm = netD(real_cpu,chord_cpu,batch_size,pitch_range)\n",
    "\n",
    "                #####loss\n",
    "                d_loss_real = reduce_mean(sigmoid_cross_entropy_with_logits(D_logits, 0.9*torch.ones_like(D)))\n",
    "                d_loss_real.backward()\n",
    "                D_x = D.mean().item()\n",
    "                sum_D_x += D_x \n",
    "\n",
    "                # train with fake\n",
    "                noise = torch.randn(batch_size, nz, device=device)\n",
    "                fake = netG(noise,prev_data_cpu,chord_cpu,batch_size,pitch_range)\n",
    "                label.fill_(fake_label)\n",
    "                D_, D_logits_, fm_ = netD(fake.detach(),chord_cpu,batch_size,pitch_range)\n",
    "                d_loss_fake = reduce_mean(sigmoid_cross_entropy_with_logits(D_logits_, torch.zeros_like(D_)))\n",
    "\n",
    "                d_loss_fake.backward()\n",
    "                D_G_z1 = D_.mean().item()\n",
    "                errD = d_loss_real + d_loss_fake\n",
    "                errD = errD.item()\n",
    "                lossD_list_all.append(errD)\n",
    "                sum_lossD += errD\n",
    "                optimizerD.step()\n",
    "\n",
    "                ############################\n",
    "                # (2) Update G network: maximize log(D(G(z)))\n",
    "                ###########################\n",
    "                # netG.zero_grad()\n",
    "                # label.fill_(real_label)  # fake labels are real for generator cost\n",
    "                # D_, D_logits_, fm_= netD(fake,chord_cpu,batch_size,pitch_range)\n",
    "\n",
    "                # ###loss\n",
    "                # g_loss0 = reduce_mean(sigmoid_cross_entropy_with_logits(D_logits_, torch.ones_like(D_)))\n",
    "                # #Feature Matching\n",
    "                # features_from_g = reduce_mean_0(fm_)\n",
    "                # features_from_i = reduce_mean_0(fm)\n",
    "                # fm_g_loss1 =torch.mul(l2_loss(features_from_g, features_from_i), 0.1)\n",
    "\n",
    "                # mean_image_from_g = reduce_mean_0(fake)\n",
    "                # smean_image_from_i = reduce_mean_0(real_cpu)\n",
    "                # fm_g_loss2 = torch.mul(l2_loss(mean_image_from_g, smean_image_from_i), 0.01)\n",
    "\n",
    "                # errG = g_loss0 + fm_g_loss1 + fm_g_loss2\n",
    "                # errG.backward(retain_graph=True)\n",
    "                # D_G_z2 = D_.mean().item()\n",
    "                # optimizerG.step()\n",
    "              \n",
    "                ############################\n",
    "                # (3) Update G network again: maximize log(D(G(z)))\n",
    "                ###########################\n",
    "                netG.zero_grad()\n",
    "                label.fill_(real_label)  # fake labels are real for generator cost\n",
    "                D, D_logits, fm = netD(real_cpu,chord_cpu,batch_size,pitch_range)\n",
    "                D_, D_logits_, fm_ = netD(fake,chord_cpu,batch_size,pitch_range)\n",
    "\n",
    "                ###loss\n",
    "                g_loss0 = reduce_mean(sigmoid_cross_entropy_with_logits(D_logits_, torch.ones_like(D_)))\n",
    "                #Feature Matching\n",
    "                features_from_g = reduce_mean_0(fm_)\n",
    "                features_from_i = reduce_mean_0(fm)\n",
    "                loss_ = nn.MSELoss(reduction='sum')\n",
    "                feature_l2_loss = loss_(features_from_g, features_from_i)/2\n",
    "                fm_g_loss1 =torch.mul(feature_l2_loss, 0.1)\n",
    "\n",
    "                mean_image_from_g = reduce_mean_0(fake)\n",
    "                smean_image_from_i = reduce_mean_0(real_cpu)\n",
    "                mean_l2_loss = loss_(mean_image_from_g, smean_image_from_i)/2\n",
    "                fm_g_loss2 = torch.mul(mean_l2_loss, 0.01)\n",
    "                errG = g_loss0 + fm_g_loss1 + fm_g_loss2\n",
    "                sum_lossG +=errG\n",
    "                errG.backward()\n",
    "                lossG_list_all.append(errG.item())\n",
    "\n",
    "                D_G_z2 = D_.mean().item()\n",
    "                sum_D_G_z += D_G_z2\n",
    "                optimizerG.step()\n",
    "            \n",
    "                if epoch % 5 == 0:\n",
    "                    print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f'\n",
    "                          % (epoch, epochs, i, len(train_loader),\n",
    "                             errD, errG, D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "                if i % 100 == 0:\n",
    "                    vutils.save_image(real_cpu,\n",
    "                            '%s/real_samples.png' % 'file',\n",
    "                            normalize=True)\n",
    "                    fake = netG(fixed_noise,prev_data_cpu,chord_cpu,batch_size,pitch_range)\n",
    "                    vutils.save_image(fake.detach(),\n",
    "                            '%s/fake_samples_epoch_%03d.png' % ('file', epoch),\n",
    "                            normalize=True)\n",
    "\n",
    "            average_lossD = (sum_lossD / len(train_loader.dataset))\n",
    "            average_lossG = (sum_lossG / len(train_loader.dataset))\n",
    "            average_D_x = (sum_D_x / len(train_loader.dataset))\n",
    "            average_D_G_z = (sum_D_G_z / len(train_loader.dataset))\n",
    "\n",
    "            lossD_list.append(average_lossD)\n",
    "            lossG_list.append(average_lossG)            \n",
    "            D_x_list.append(average_D_x)\n",
    "            D_G_z_list.append(average_D_G_z)\n",
    "\n",
    "            # do checkpointing\n",
    "            torch.save(netG.state_dict(), '%s/netG_epoch_%d.pth' % ('models', epoch))\n",
    "            torch.save(netD.state_dict(), '%s/netD_epoch_%d.pth' % ('models', epoch))\n",
    "            print('==> Epoch: {} Average lossD: {:.10f} average_lossG: {:.10f},average D(x): {:.10f},average D(G(z)): {:.10f} '.format(\n",
    "              epoch, average_lossD,average_lossG,average_D_x, average_D_G_z)) \n",
    "\n",
    "        np.save('lossD_list.npy',lossD_list)\n",
    "        np.save('lossG_list.npy',lossG_list)\n",
    "        np.save('lossD_list_all.npy',lossD_list_all)\n",
    "        np.save('lossG_list_all.npy',lossG_list_all)\n",
    "        np.save('D_x_list.npy',D_x_list)\n",
    "        np.save('D_G_z_list.npy',D_G_z_list)\n",
    "        \n",
    "\n",
    "    if is_draw == 1:\n",
    "        lossD_print = np.load('lossD_list.npy')\n",
    "        lossG_print = np.load('lossG_list.npy')\n",
    "        length = lossG_print.shape[0]\n",
    "\n",
    "        x = np.linspace(0, length-1, length)\n",
    "        x = np.asarray(x)\n",
    "        plt.figure()\n",
    "        plt.plot(x, lossD_print,label=' lossD',linewidth=1.5)\n",
    "        plt.plot(x, lossG_print,label=' lossG',linewidth=1.5)\n",
    "\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.xlabel('data')\n",
    "        plt.ylabel('loss')\n",
    "        plt.savefig('where you want to save/lr='+ str(lr) +'_epoch='+str(epochs)+'.png')\n",
    "\n",
    "    if is_sample == 1:\n",
    "        batch_size = 8\n",
    "        nz = 100\n",
    "        n_bars = 7\n",
    "        X_te = np.load('data/tst_x.npy')\n",
    "        prev_X_te = np.load('data/tst_x_prev.npy')\n",
    "        prev_X_te = prev_X_te[:,:,check_range_st:check_range_ed,:]\n",
    "        y_te    = np.load('data/chord_tst.npy')\n",
    "       \n",
    "        test_iter = get_dataloader(X_te,prev_X_te,y_te)\n",
    "        kwargs = {'num_workers': 4, 'pin_memory': True}# if args.cuda else {}\n",
    "        test_loader = DataLoader(test_iter, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "        netG = sample_generator(pitch_range)\n",
    "        netG.load_state_dict(torch.load('models/netG_epoch_19.pth'))\n",
    "\n",
    "        output_songs = []\n",
    "        output_chords = []\n",
    "        for i, (data,prev_data,chord) in enumerate(test_loader, 0):\n",
    "            list_song = []\n",
    "            first_bar = data[0].view(1,1,16,128)\n",
    "            list_song.append(first_bar)\n",
    "\n",
    "            list_chord = []\n",
    "            first_chord = chord[0].view(1,13).cpu().numpy()\n",
    "            list_chord.append(first_chord)\n",
    "            noise = torch.randn(batch_size, nz)\n",
    "\n",
    "            for bar in range(n_bars):\n",
    "                z = noise[bar].view(1,nz)\n",
    "                y = chord[bar].view(1,13)\n",
    "                if bar == 0:\n",
    "                    prev = data[0].view(1,1,16,128)\n",
    "                else:\n",
    "                    prev = list_song[bar-1].view(1,1,16,128)\n",
    "                sample = netG(z, prev, y, 1,pitch_range)\n",
    "                list_song.append(sample)\n",
    "                list_chord.append(y.numpy())\n",
    "\n",
    "            print('num of output_songs: {}'.format(len(output_songs)))\n",
    "            output_songs.append(list_song)\n",
    "            output_chords.append(list_chord)\n",
    "        np.save('output_songs.npy',np.asarray(output_songs))\n",
    "        np.save('output_chords.npy',np.asarray(output_chords))\n",
    "\n",
    "        print('creation completed, check out what I make!')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\" :\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8e5b3d-9313-47c4-9d83-5ffafd6a93cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
